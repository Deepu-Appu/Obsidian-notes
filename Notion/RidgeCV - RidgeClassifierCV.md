These are smart version of `Ridge` and `RidgeClassifier` that `automatically` choose the best alpha **value** for using **cross-validation**.

### üß† ==What is Cross-Validation?==

It‚Äôs like:

> ‚ÄúLet me split the data into parts. train on some , test on others, and repeat ‚Äî to find the best `alpha that works well on new data`.‚Äù

###   üìò  `RidgeCV` ‚Äì ==Ridge Regression with Cross-Validation==

This class helps you automatically choose the **best alpha value** (penalty strength) using cross-validation

It tries multiple `alpha` value and `picks the one that gives the best prediction results.

|Parameter|Meaning in Simple Terms|
|---|---|
|`**alphas=(0.1, 1.0, 10.0)**`|These are the **different alpha values** RidgeCV will try. It picks the one that performs best during cross-validation.|
|`**fit_intercept=True**`|Should the model **learn the y-axis intercept** (starting value when all inputs are 0)? Usually, this is set to `True`.|
|`**scoring=None**`|How to **measure performance**? Leave as `None` to use default (least squares). You can also use `'neg_mean_squared_error'`, `'r2'`, etc.|
|`**cv=None**`|How to **split the data** for cross-validation? If left as `None`, RidgeCV uses an efficient method called **Leave-One-Out CV** by default. You can also pass a number (like 5) for **K-Fold CV**.|
|`**gcv_mode=None**`|Applies only if `cv=None`. It decides how to do Generalized Cross Validation. Usually, you don‚Äôt need to set this manually.|
|`**store_cv_results=False**`|If `True`, the model will **store the cross-validation scores** for all alpha values ‚Äî useful if you want to analyze how each alpha performed.|
|`**alpha_per_target=False**`|Only used for **multi-output regression** (multiple target values). If `True`, RidgeCV will choose a different alpha for each target.|

  

### üìò `RidgeClassifierCV` ‚Äì ==Ridge Regression with Cross-Validation==

It is like `RidgeCV` , but for **classification tasks** (not regression).

It:

- Converts class labels to `{-1, 1}` internally
- Trains using Ridge (L2-regularized least squares)
- Automatically picks the best `alpha using cross-validation

|Parameter|Meaning|
|---|---|
|`alphas`|A list of values to try for the regularization strength (e.g. 0.1, 1.0). The model will test all of these and pick the best one.|
|`fit_intercept`|If `True`, the model will also learn the y-axis starting point. Usually leave it as `True`.|
|`scoring`|How to measure performance. If left `None`, it uses **accuracy**. You can change this to use precision, recall, etc.|
|`cv`|Number of cross-validation splits. `cv=5` means split the training data into 5 parts and validate.|
|`class_weight`|Helps handle **imbalanced datasets**. Set to `"balanced"` to give more weight to less frequent classes.|
|`store_cv_results`|If `True`, keeps the full cross-validation results so you can analyze them later with `.cv_values_`.|